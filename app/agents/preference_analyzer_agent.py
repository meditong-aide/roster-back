import json
from pydantic import BaseModel
from typing import List, TypedDict, Annotated, operator
from google import genai
from google.genai import types
from langgraph.graph import StateGraph, END
from langchain_anthropic import ChatAnthropic
from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import SystemMessage, HumanMessage
import os
import dotenv
try:
    import tiktoken
except Exception:
    tiktoken = None

dotenv.load_dotenv()

class PreferenceSubgraph(TypedDict):
    requests: List[str]
    n_requests: int
    phase: int
    schema: object
    preference_result: Annotated[list, operator.add]
    model: object


def collector(state):
    """
    information collector
    """


def init_data(state):
    """
    initialize data
    """


# ------------------------------
# ë¹„ìš©/í† í° ìœ í‹¸ë¦¬í‹°
# ------------------------------
def _krw_per_usd() -> float:
    try:
        return float(os.getenv("KRW_PER_USD", "1350"))
    except Exception:
        return 1350.0


def _pricing_per_1k(model_name: str) -> tuple[float, float]:
    """
    ëª¨ë¸ë³„ 1K í† í°ë‹¹ (ì…ë ¥, ì¶œë ¥) USD ë¹„ìš©ì„ ë°˜í™˜.
    """
    name = (model_name or "").lower()
    input_usd, output_usd = 0.003, 0.009
    if "gpt-4o" in name:
        input_usd, output_usd = 0.005, 0.015
    elif "claude" in name:
        input_usd, output_usd = 0.003, 0.015
    elif "gemini" in name:
        input_usd, output_usd = 0.00035, 0.00105
    return input_usd, output_usd


def _encoding_for_model(model_name: str):
    name = (model_name or "").lower()
    try:
        if "gpt" in name or "o" in name:
            return tiktoken.get_encoding("o200k_base")
        return tiktoken.get_encoding("cl100k_base")
    except Exception:
        return None


def _count_tokens(text: str, model_name: str) -> int:
    if not text:
        return 0
    enc = _encoding_for_model(model_name)
    if enc is None:
        return max(1, int(len(text) / 4))
    try:
        return len(enc.encode(text))
    except Exception:
        return max(1, int(len(text) / 4))


def _count_messages_tokens(messages: List[str], model_name: str) -> int:
    return sum(_count_tokens(m or "", model_name) for m in messages)


def _compute_cost(prompt_tokens: int, completion_tokens: int, model_name: str) -> dict:
    in_per_1k, out_per_1k = _pricing_per_1k(model_name)
    input_usd = (prompt_tokens / 1000.0) * in_per_1k
    output_usd = (completion_tokens / 1000.0) * out_per_1k
    total_usd = input_usd + output_usd
    rate = _krw_per_usd()
    return {
        "model": model_name,
        "pricing_per_1k_usd": {"input": in_per_1k, "output": out_per_1k},
        "usage": {
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens,
            "total_tokens": prompt_tokens + completion_tokens,
        },
        "cost_usd": {
            "input": round(input_usd, 6),
            "output": round(output_usd, 6),
            "total": round(total_usd, 6),
        },
        "cost_krw": {
            "input": int(round(input_usd * rate)),
            "output": int(round(output_usd * rate)),
            "total": int(round(total_usd * rate)),
            "krw_per_usd": rate,
        },
    }


class preferenceAnalyzer(BaseModel):
    processor: str
    id : str
    weight : float
    reason : str
    request : str

class preferenceAnalyzerPrompt:
    def __init__(self, schem, query):
        """
        í”„ë¡¬í”„íŠ¸ í´ë˜ìŠ¤
        """
        self.system = f"""
            # ì—­í• 
            ë‹¹ì‹ ì€ "ê°„í˜¸ì‚¬ ê·¼ë¬´í‘œ ì—”ì§„"ìš© **ì„ í˜¸ ìŠ¤ì½”ì–´ ì¶”ì¶œê¸°**ì…ë‹ˆë‹¤.  
            ì…ë ¥ìœ¼ë¡œ ìì—°ì–´ ë¬¸ì¥(í•œêµ­ì–´Â·ì˜ì–´Â·í˜¼í•©)ì„ ë°›ìœ¼ë©´, ê°„í˜¸ì‚¬ ê°„ pair-score ì™€ ê°œì¸ shift-score ë¡œ ë³€í™˜í•´ JSON ìœ¼ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.

            # ì…ë ¥ ìŠ¤í‚¤ë§ˆ
            - nurses : ê°ì²´ ë°°ì—´  
            ```json
            {{ "id": int, "name": str, "exp": float,
                "is_head": bool, "is_night_nurse": int }}
            ```
            * utterances : ë¬¸ìì—´ ë°°ì—´
                (ê°„í˜¸ì‚¬ë“¤ì˜ "ê°™ì´ í•˜ê³  ì‹¶ì–´/ì‹«ì–´, ê²¹ì¹˜ì§€ ë§ì•„ì¤˜" ë“± ììœ  ì„œìˆ )

            # ì¶œë ¥ ìŠ¤í‚¤ë§ˆ
            ```json
            {{
                "processor": "ë¶„ì„ ê³¼ì • ì„¤ëª…",
                "id": "nurse_id",
                "weight": 0.0,
                "reason": "ì„ í˜¸/ê¸°í”¼ ì´ìœ "
            }}
            ```

            # ê°€ì´ë“œë¼ì¸
                1. ë§¤í•‘ ê·œì¹™
                    | í‘œí˜„ íŒ¨í„´        | ì˜ˆì‹œ                       | weight               |
                    | ------------ | ------------------------ | -------------------- |
                    | **ê°•í•œ ì„ í˜¸**    | "ê¼­ â—‹â—‹ìŒ¤ì´ë‘" "ë¬´ì¡°ê±´ ê°™ì´"       | +3.0                 |
                    | **ë³´í†µ ì„ í˜¸**    | "ê°€ëŠ¥í•˜ë©´ â—‹â—‹ìŒ¤" "ê°™ì´ í•˜ê³  ì‹¶ì–´"    | +1.5                 |
                    | **ë³´í†µ ê¸°í”¼**    | "ê°€ê¸‰ì  â—‹â—‹ìŒ¤ì€ í”¼í•˜ê³ "           | âˆ’1.5                 |
                    | **ê°•í•œ ê¸°í”¼**    | "ì ˆëŒ€ â—‹â—‹ìŒ¤ì´ë‘ ì‹«ì–´" "ì œë°œ ì•ˆ ê²¹ì¹˜ê²Œ" | âˆ’2.0                 |
                    | **ëª¨í˜¸/ë†ë‹´/ì—†ìŒ**    | "â—‹â—‹ìŒ¤ì´ë‘ì€ ê¸€ì„ìš”ã…ã…"           | 0 |
                3. ê·œì¹™
                    * id ë§¤í•‘ì€ ì´ë¦„ ì™„ì „ì¼ì¹˜ ìš°ì„ , ì´ë¦„ë„, ì„±ë„ ì°¾ì§€ ëª»í•˜ë©´ ignored(0).
                    * íŒ©íŠ¸ ì—†ëŠ” ì¶”ë¡ Â·í™˜ìƒ (hallucination)ì€ ê¸ˆì§€.
                    * ì •ê·œí™”Â·í›„ì²˜ë¦¬ëŠ” ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì—”ì§„ì´ ìˆ˜í–‰í•˜ë¯€ë¡œ weight ë²”ìœ„ë§Œ ì§€ì¼œë¼.
                4. ì˜ˆì‹œ ì…ì¶œë ¥
                    <ì…ë ¥>
                    ```json
                        {{
                        "nurses":[
                        {{"nurse_id":"slfnam1","name":"ê¹€ê°€í¬","exp":6,"is_head":false,"is_night_nurse":0}},
                        {{"nurse_id":"mlnwjk2","name":"ë°•ìˆ˜ì •","exp":3,"is_head":false,"is_night_nurse":1}},
                        {{"nurse_id":"ooonsjk3","name":"ì´í•´ë¦°","exp":10,"is_head":true,"is_night_nurse":0}}
                        ],
                        "utterances":[
                        "ì € ë°•ìˆ˜ì • ìŒ¤ì´ë‘ì€ ì œë°œ ì•ˆ ê²¹ì¹˜ê²Œ í•´ì£¼ì„¸ìš”â€¦ğŸ˜­"
                        ]
                        }}
                    ```
                    <ì¶œë ¥>
                    ```json
                        {{
                        "processor": "'ë°•ìˆ˜ì • ìŒ¤'ì€ ì •ë³´ìƒ nurse_idê°€ 'mlnwjk2'ì´ê³ , ê°•í•œ ê¸°í”¼ë¥¼ í‘œí˜„í•˜ë‹ˆ ê°€ì¤‘ì¹˜ëŠ” -2ë¡œ ì¤˜ì•¼í•  ê²ƒ ê°™ì•„.",
                        "id": "mlnwjk2",
                        "weight": -2.0,
                        "reason": "ê°•í•œ ê¸°í”¼ í‘œí˜„",
                        "request": "ì € ë°•ìˆ˜ì • ìŒ¤ì´ë‘ì€ ì œë°œ ì•ˆ ê²¹ì¹˜ê²Œ í•´ì£¼ì„¸ìš”â€¦ğŸ˜­"
                        }}
                    ```
                5. ì¶œë ¥ í˜•ì‹
                    * ë°˜ë“œì‹œ ìœ„ JSON êµ¬ì¡°ë§Œ ë°˜í™˜ (ë¶ˆí•„ìš”í•œ ë¬¸ì¥Â·ì£¼ì„ x)
                    * ì—¬ëŸ¬ ëª…ì´ ì–¸ê¸‰ë˜ë©´ ê°€ì¥ ì¤‘ìš”í•œ/ëª…í™•í•œ í•œ ëª…ë§Œ ì„ íƒ
            """
                
        self.human=f"""
            # INPUT SCHEMA: 
                {schem}
                * utterances:
                {query} 
            # OUTPUT:
            """

async def preference_analyzer(state):
    
    phase = state['phase']
    query = state['requests'][phase]
    data = state['schema']
    
    preference_analyzer_prompt = preferenceAnalyzerPrompt(data, query)
    
    # ë°±ì—… ëª¨ë¸ë“¤ ìˆœì„œëŒ€ë¡œ ì‹œë„
    models_to_try = [
        # 1ì°¨: OpenAI (ê¸°ë³¸)
        ChatOpenAI(
            model="gpt-4.1-mini-2025-04-14",
            openai_api_key=os.getenv("OPENAI_API_KEY"),
        ),
        # 2ì°¨: Anthropic (ë°±ì—…)
        ChatAnthropic(
            model="claude-3-7-sonnet-20250219",
            anthropic_api_key=os.getenv("ANTHROPIC_API_KEY"),
        ),
        # 3ì°¨: Google Gemini (ìµœì¢… ë°±ì—…)
        ChatGoogleGenerativeAI(
            model="gemini-2.0-flash",
            google_api_key=os.getenv("GOOGLE_API_KEY"),
        )
    ]
    
    messages = [
        SystemMessage(content=preference_analyzer_prompt.system),
        HumanMessage(content=preference_analyzer_prompt.human)
    ]
    
    json_answer = {
        "processor": "ê¸°ë³¸ê°’ ì„¤ì •",
        "id": "",
        "weight": 0.0,
        "reason": "ì²˜ë¦¬ ì‹¤íŒ¨",
        "request": query
    }
    used_model_name = ""
    
    for i, client in enumerate(models_to_try):
        try:
            
            print(f"Preference Analyzer: {i+1}ì°¨ ëª¨ë¸ ì‹œë„ ì¤‘...")
            
            llm = client.with_structured_output(preferenceAnalyzer)

            response = await llm.ainvoke(messages)
            used_model_name = getattr(client, "model", "") or used_model_name
            print('used_model_name', used_model_name)

            # ì„±ê³µ ì‹œ ë°ì´í„° ì¶”ì¶œ
            json_answer = {
                "processor": response.processor,
                "id": response.id,
                "weight": response.weight,
                "reason": response.reason,
                "request": query
            }
            
            # ID ê²€ì¦: schemaì— ì¡´ì¬í•˜ëŠ” ê°„í˜¸ì‚¬ì¸ì§€ í™•ì¸
            valid_nurse_ids = []
            if isinstance(data, list):
            
                valid_nurse_ids = [nurse.get('nurse_id', '') for nurse in data if isinstance(nurse, dict)]
            elif isinstance(data, dict) and 'nurses' in data:
            
                nurses = data.get('nurses', [])
                valid_nurse_ids = [nurse.get('nurse_id', '') for nurse in nurses if isinstance(nurse, dict)]
            
            # ì¶”ì¶œëœ idê°€ ìœ íš¨í•œ ê°„í˜¸ì‚¬ IDê°€ ì•„ë‹ˆë©´ ë¹ˆ ê°’ìœ¼ë¡œ ì²˜ë¦¬
            if json_answer["id"] and json_answer["id"] not in valid_nurse_ids:
                print(f"Preference Analyzer: ë¬´íš¨í•œ ê°„í˜¸ì‚¬ ID '{json_answer['id']}' - ë¹ˆ ê°’ìœ¼ë¡œ ì²˜ë¦¬")
                json_answer = {
                    "processor": f"ì–¸ê¸‰ëœ ê°„í˜¸ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ë¬´ì‹œë¨: {response.processor}",
                    "id": "",
                    "weight": 0.0,
                    "reason": "í•´ë‹¹í•˜ëŠ” ê°„í˜¸ì‚¬ê°€ ìŠ¤í‚¤ë§ˆì— ì¡´ì¬í•˜ì§€ ì•ŠìŒ",
                    "request": query
                }
            
            print(f"Preference Analyzer: {i+1}ì°¨ ëª¨ë¸ ì„±ê³µ!")
            break
            
        except Exception as e:
            error_msg = str(e).lower()
            print(f"Preference Analyzer: {i+1}ì°¨ ëª¨ë¸ ì˜¤ë¥˜ - {e}")
            
            # 429 (Rate limit) ë˜ëŠ” 529 (Service unavailable) ì—ëŸ¬ì¸ì§€ í™•ì¸
            if ("429" in error_msg or "rate" in error_msg or 
                "529" in error_msg or "service unavailable" in error_msg or
                "quota" in error_msg or "limit" in error_msg):
                
                if i < len(models_to_try) - 1:
                    print(f"Preference Analyzer: {i+2}ì°¨ ë°±ì—… ëª¨ë¸ë¡œ ì¬ì‹œë„...")
                    continue
                else:
                    print("Preference Analyzer: ëª¨ë“  ë°±ì—… ëª¨ë¸ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©")
                    break
            else:
                # ë‹¤ë¥¸ ì—ëŸ¬ëŠ” ì¦‰ì‹œ ë°±ì—… ëª¨ë¸ë¡œ ì‹œë„
                if i < len(models_to_try) - 1:
                    print(f"Preference Analyzer: ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜, {i+2}ì°¨ ë°±ì—… ëª¨ë¸ë¡œ ì¬ì‹œë„...")
                    continue
                else:
                    print("Preference Analyzer: ëª¨ë“  ëª¨ë¸ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©")
                    break
    
    # í† í°/ë¹„ìš© ê³„ì‚°
    model_name_for_calc = used_model_name or (getattr(models_to_try[0], "model", "") or "")
    prompt_tokens = _count_messages_tokens([preference_analyzer_prompt.system, preference_analyzer_prompt.human], model_name_for_calc)
    completion_json = json.dumps(json_answer, ensure_ascii=False)
    completion_tokens = _count_tokens(completion_json, model_name_for_calc)
    cost_info = _compute_cost(prompt_tokens, completion_tokens, model_name_for_calc)
    print(f"í† í° ì‚¬ìš©ëŸ‰(Preference): {cost_info['usage']}, ë¹„ìš©(USD/KRW): {cost_info['cost_usd']} / {cost_info['cost_krw']}")
    
    return {'preference_result': [json_answer]}


async def create_preference_analyzer(parent_state):
    requests = parent_state['query_preference']         # Shift List ex. ["9/9: D", "9/10: D", "9/16: OFF", "9/9, 9/10, 9/16 ì™¸ì— ì›¬ë§Œí•˜ë©´ Eë¡œ ì¤˜"]
    schema = parent_state['schema']
    client = parent_state['model']
    n_requests = len(requests)
    if n_requests == 0:
        return {"preference_results": []}
    graph = StateGraph(PreferenceSubgraph)
    
    graph.add_node("init_data", init_data)
    graph.add_node("collector", collector)
    graph.set_entry_point('init_data')
    for n in range(n_requests):
        def create_preference_node(n):
            async def wrapped_shift(state):
                state['phase']= n
                return await preference_analyzer(state)
            return wrapped_shift
        graph.add_node('preference_analyzer' +str(n), create_preference_node(n))
        graph.add_edge('init_data', 'preference_analyzer' +str(n))
        graph.add_edge('preference_analyzer'+ str(n), "collector")

    graph.add_edge('collector', END)
    graph_app = graph.compile()

    result = await graph_app.ainvoke({"requests": requests, "schema": schema, "model": client})
    print(f'\n\n\n\n\npreference_results, {result}\n\n\n\n\n')
    return {"preference_results": [result]}

